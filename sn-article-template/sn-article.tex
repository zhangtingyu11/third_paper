%Version 3 October 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove “Numbered” in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst%  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-basic]{sn-jnl}% Math and Physical Sciences Numbered Reference Style 
%%\documentclass[sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>

\usepackage{graphicx}%
\usepackage{multirow}%

\usepackage{multicol, lipsum}
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{amssymb}%
\usepackage{graphics}
\usepackage{adjustbox}
\usepackage{tabularx,ragged2e}
\usepackage[section]{placeins}
\usepackage{hyperref}

\newcommand{\R}{\mathbb{R}}

%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Probabilistic Refinement for RoI-based Monocular 3D Object Detection}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1]{\fnm{Tingyu} \sur{Zhang}}\email{zhangty21@mails.jlu.edu.cn}

\author[2]{\fnm{Xinyu} \sur{Yang}}\email{yangxy18@mails.jlu.edu.cn}

\author[1]{\fnm{Zhigang} \sur{Liang}}\email{liangzg22@mails.jlu.edu.cn}
\author[1]{\fnm{Yanzhao} \sur{Yang}}\email{yangyz23@mails.jlu.edu.cn}
\author*[1]{\fnm{Jian} \sur{Wang}}\email{wangjian591@jlu.edu.cn}


\affil*[1]{\orgdiv{College of Computer Science and Technology}, \orgname{Jilin University}, \orgaddress{\street{No. 2699 Qianjin Avenue}, \city{Changchun}, \postcode{130012}, \state{Jilin}, \country{China}}}

\affil[2]{\orgdiv{Simulation Test Department}, \orgname{China Automative Innovation Corporation}, \orgaddress{\street{No. 88 Shengli Avenue}, \city{Nanjing}, \postcode{210000}, \state{Jiangsu}, \country{China}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{Monocular 3D object detection has garnered significant attention due to its cost-effectiveness and simplified setup. In this study, we delve into Region of Interest (RoI)-based monocular detectors. Previous approaches treat all parts of the RoI equally. However, different regions within the RoI hold varying importance, and accurate RoI estimation may be hindered by occlusions or long distances. Thus, we introduce the Multi-Scale Grid Attention (MSGA) mechanism to investigate multi-scale RoI exploration and the significance of RoI regions. Moreover, while existing methods treat depth estimation as a probability estimation during training, they do not effectively utilize probabilistic properties during inference. To tackle these issues, we propose a novel probabilistic post-processing method to enhance detection robustness. Experimental evaluations are conducted on the KITTI and Waymo datasets, achieving state-of-the-art performance.}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Camera, Intelligent vehicles, Monocular 3D object detection, Image Processing}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{sec1}

{M}{onocular} camera-based 3D object detection has become a significant research area in computer vision, with applications spanning autonomous driving, robotics, and augmented reality. Unlike conventional approaches dependent on depth sensors or multi-camera setups~\cite{segment-fusion, srif}, monocular 3D object detection leverages the capabilities of a single camera to estimate the three-dimensional spatial information of objects within its field of view.

The significance of monocular 3D object detection lies in its potential to provide depth perception using only the information captured by a single camera. This is particularly advantageous for real-world applications~\cite{teng} where cost, simplicity, and computational efficiency are essential considerations. Achieving accurate and robust 3D object detection from monocular images involves addressing complex challenges such as scale ambiguity, occlusions, and perspective distortions inherent to a single viewpoint.

The absence of explicit depth information poses a significant challenge for monocular 3D detectors, leading to a considerable disparity compared to point cloud-based methods. To bridge this gap, certain approaches~\cite{gupnet, didm3d, neurocs} initially forecast multiple RoI candidates and subsequently generate predictions for each. 3D RoI-based methods, such as those discussed in~\cite{neurocs}, often depend on pre-trained 3D monocular detectors. In contrast, 2D RoI-based methods solely predict 2D bounding boxes, offering a lighter and more efficient alternative. Furthermore, given the real-time demands of 3D object detection, efficiency is paramount. Therefore, this paper exclusively focuses on 2D RoI-based methods.

2D RoI-based methods~\cite{gupnet, didm3d} typically involve three key steps. Firstly, the RoI is partitioned into distinct segments, followed by the application of RoI feature extraction techniques (such as RoI Pooling~\cite{fast-rcnn}, RoI Warp~\cite{roi-warp}, and RoI Align~\cite{mask-rcnn}) to extract pertinent features. Secondly, the RoI is fed into a neural network to further refine these features and generate predictions for each region. Finally, the individual predictions are integrated to yield the final results.

However, the second and third steps present certain challenges. In the second step, each partition of the RoI is treated equally in previous works, disregarding the varying importance of different RoI regions. For instance, empirically, information pertaining to the tire or license plate number holds more significance than that concerning the car body, and background regions should have less influence than foreground areas. Taking DID-M3D~\cite{didm3d} as an example, it predicts the depth to the object surface, referred to as visual depth, to decouple the depth. In the KITTI~\cite{kitti} dataset, the training data for the depth completion task and 3D object detection task do not entirely overlap, resulting in some images in KITTI 3D lacking ground truth visual depth. Therefore, DID-M3D introduces a depth completion network to generate a dense visual depth map. We select images with depth labels and compare the ground truth with the depth map generated by the depth completion network. As depicted in Fig.~\ref{fig:depth_residual}, visual depth near the center of an object is more precise than at the edges. Taking the central visual depth of the RoI as the baseline, the distribution of visual depth in the RoI is imbalanced, as shown in Fig.~\ref{fig:visual_depth_offset}. Additionally, 2D RoIs are generated by the network, potentially resulting in incomplete object warping. To address these issues, we propose the MSGA module. Specifically, we enlarge the RoI by adding various pixels to its size, extract multi-scale features using a network, and apply a Grid Attention (GA) module to assess the importance of different regions and further adjust the RoI features.
\begin{figure}[!t]
	\centering
	\includegraphics[width=1.0\linewidth]{Fig1}
	\caption{Instance and its depth residual. The depth residual of an instance is computed by taking the absolute difference between the predicted depth generated by the depth completion network and the corresponding ground truth. The left column shows the raw image of the instance, while the right column illustrates the depth residual. Lighter colors in the latter column indicate higher depth residuals.}
	\label{fig:depth_residual}
\end{figure}
\begin{figure}[!t]
	\centering
	\includegraphics[width=1.0\linewidth]{Fig2.eps}
	\caption{Visual depth offset. The visual depth of the central grid within the RoI is considered the baseline, from which the offset to other grids is computed. This absolute offset is subsequently labeled for each grid.}
	\label{fig:visual_depth_offset}
\end{figure}
\begin{figure}[!t]
	\centering
	\includegraphics[width=1.0\linewidth]{Fig3.eps}
	\caption{Decoupled depth. The instance depth is defined as the distance between the object center to the camera plane. And the visual depth is distance of object surface to the camera plane. The attribute depth is obtained by the minus of instance depth and visual depth.}
	\label{fig:decoupled depth}
\end{figure}
In the third step, the results are integrated to obtain the final outcomes. Previous approaches commonly employed simple averaging or weighted summation. While adequate for properties like dimensions and position, where each grid shares the same regression target, this method encounters challenges in depth estimation. In particular, depth is decomposed into visual depth and attribute depth~\cite{didm3d}, as illustrated in Fig.~\ref{fig:decoupled depth}. Each grid possesses distinct visual and attribute depths, leading to an imbalanced distribution that renders simple averaging suboptimal. Moreover, deriving the weights for weighted summation proves challenging. In our study, we adopt the assumption from previous research~\cite{gupnet, didm3d} that depth follows a certain distribution, leveraging this probability distribution in the post-processing phase. This approach enhances the integration process's rationality.


In summary, our contributions can be outlined as follows:
\begin{itemize}
	\item We meticulously explore the limitations of 2D RoI-based monocular 3D object detection, highlighting an inconsistency in the significance of RoI grids. We introduce the notion of varying importance among RoI grids and propose the utilization of Multi-Scale Grid Attention to address this issue.
	\item During the post-processing phase, we meticulously consider the assumed depth distribution, employing probabilistic methods for refinement.
	\item The experimental results underscore the superiority of our method compared to existing approaches on the KITTI and Waymo datasets.
\end{itemize}

\section{Related Work}

\subsection{Monocular 3D Object Detection without RoI}

	Given the inherent challenge of directly estimating instance depth with a monocular camera due to its ill-posed nature, previous studies have endeavored to leverage these constraints effectively. Mousavian et al. \cite{geometry3d} introduced constraints between 2D and 3D bounding boxes to formulate an equation group, integrating specific priors like driving direction to constrain the equation's degrees of freedom. PGD \cite{pgd} and MonoPair \cite{monopair} explore relationships between different instances, while DDMP-3D \cite{ddmp-3d} considers the relationships between neighboring pixels, utilizing these constraints to refine box estimations.

Some alternative methods \cite{m3d-rpn, d4lcn, monodtr} employ numerous anchors placed on the 3D plane and extract anchor features through projection. These approaches encounter common issues associated with anchor-based methods, including the proliferation of anchors and the nontrivial configuration of anchor hyperparameters.

Due to the scale variance in the image plane and the notable success of LiDAR detectors, several methods endeavor to convert the 2D image plane into the 3D plane. OFT \cite{oft} and ImVoxelNet \cite{imvoxelnet} utilize the projection of predefined 3D voxels onto the image plane to populate the features of each voxel. CaDDN \cite{caddn} partitions the depth range into segments and predicts the probability for each segment, followed by the expansion of pixel features into frustum features. MonoNeRD \cite{mononerd} leverages 2D image features to generate NeRF-like 3D representations. Additionally, various pseudo-LiDAR methods \cite{color-embeded, mono-plidar, patchnet} integrate an independent depth completion network to generate a dense depth map, using the calibration matrix to derive the pseudo LiDAR. Subsequently, a LiDAR-based detector is employed to obtain the final results. Despite its success, DD3D \cite{dd3d} argues that pseudo LiDAR is unnecessary for detection. Instead, it employs a single model to predict depth and 3D bounding box simultaneously, obviating the need for an independent depth completion network.

Monocular 3D object detection heavily relies on accurate depth estimation~\cite{monodle}. Even slight errors in depth estimation can lead to objects being significantly offset from ground truth positions. To address this challenge, many methods aim to introduce additional supervision to constrain depth offsets. Key point estimation, as demonstrated in works such as Polygon~\cite{polygon}, is commonly employed because it imposes eight additional constraints on the model. Representative methods utilizing this approach include RTM3D~\cite{rtm3d}, SMOKE~\cite{smoke}, MonoDDE~\cite{monodde}, and Monocon~\cite{monocon}.

There are several methods that employ other different strategies to improve performance. For instance, M3DSSD~\cite{m3dssd}, MonoPGC~\cite{monopgc}, CIE~\cite{cie}, and SSD-MonoDETR~\cite{ssd-monodetr} incorporate attention mechanisms to enhance performance. MonoFlex~\cite{monoflex} specifically addresses truncated objects by predicting edge heatmaps for them.

\subsection{Monocular 3D Object Detection with RoI}

RoI-based object detection can be broadly categorized into 3D RoI-based and 2D RoI-based methods. 3D RoI-based approaches~\cite{neurocs, monoxiver} typically depend on a separately pretrained monocular 3D object detector to produce 3D proposals, a process that does not meet the real-time demands of autonomous driving. In contrast, 2D RoI-based methods~\cite{roi10d, didm3d, gupnet, gupnet++} share most of the network in both RoI generation and object detection, rendering them more efficient.

In this study, our emphasis is on 2D RoI-based methods. Recognizing the challenges posed by RoI generation noise and the varying significance of RoI components, we introduce the MSGA module to effectively address these issues. Additionally, we introduce a novel probabilistic post-processing strategy to leverage the probabilistic assumptions inherent in depth estimation.
\begin{figure*}[!t]
	\centering
	\includegraphics[width=1.0\linewidth]{Fig4.eps}
	\caption{Overview of our model. Initially, the input image undergoes processing through the backbone network to extract image features. These features are subsequently utilized for 2D detection, keypoint estimation, and 3D heatmap generation. Leveraging the results of 2D detection, the Region of Interest (RoI) features are directed to the Multi-Scale Grid Attention (MSGA) module for refinement. The enhanced RoI features are then employed to predict the 3D offset, observation angle, and depth. The depth estimation undergoes optimization via the Probabilistic Post-Processing (PPP) module. Finally, the results are decoded by both 3D properties and optimized depth.}
	\label{fig:overview}
\end{figure*}
\section{Method}\label{sec:method}
\subsection{Problem Definition}
In the domain of monocular 3D object detection, the primary input consists of the RGB image $I$. The main goal is to determine essential properties of the 3D bounding boxes, including the 3D center coordinates $x_c, y_c, z_c$, the 3D dimensions $l, w, h$ and the observation angle $\theta$, which is more related to image appearance than orientation angle~\cite{geometry3d}. A crucial element in this procedure is the projection matrix $P$, as described in Eq.~\eqref{eq:projection_matrix}.
\begin{equation}
	P=\begin{pmatrix}f&0&c_u&-fb_x\\0&f&c_v&-fb_y\\0&0&1&-fb_z\end{pmatrix}
	\label{eq:projection_matrix}
\end{equation}

where $f$ denotes the focal length, while $c_u$ and $c_v$ denote the vertical and horizontal positions of the camera in the image, respectively. Furthermore, $b_x$, $b_y$ and $b_z$ indicate the baseline relative to the reference camera. Notably, these values are non-zero in the KITTI dataset and zero in the Waymo dataset.

In a monocular setting, directly determining the 3D center position poses a significant challenge, mainly due to the considerable variability in 3D center scale. As a result, many studies opt to predict the projected 3D center on the image plane, denoted as $x_{ic}, y_{ic}$, along with the corresponding depth $d$. The recovery of the 3D bounding box center is then accomplished using Eq.~\eqref{eq:projection}.
\begin{equation}
	dx_{2d} = Px_{3d}
	\label{eq:projection}
\end{equation}

where $P$ denotes the projection matrix, $x_{3d}$ represents the homogeneous 3D bounding box center $(x_c, y_c, z_c, 1)^T$, $x_{2d}$ signifies the homogeneous projected 3D center on the image plane $(x_{ic}, y_{ic}, 1)^T$, and $d$ denotes the depth of $x_{3d}$.

\subsection{Overview}
The schematic overview of our methodology is depicted in Fig.~\ref{fig:overview}. Initially, the provided image $I$ is processed through the image backbone, as discussed in Section~\ref{backbone}. Subsequently, a 2D detection head is employed to extract 2D properties, including the width and height of the bounding box, and to generate the heatmap for the projected 3D object center, as elaborated in Section~\ref{2d_detection_head}. Key point estimation, following the methodology of Monocon~\cite{monocon}, is incorporated into our approach. Utilizing the heatmap in conjunction with the predicted width and height, the 2D RoIs are determined. The RoIs are then input into the MSGA module to refine the features, as explained in Section~\ref{msga}. Employing the DID-M3D~\cite{didm3d} approach for each RoI grid, we predict both the visual depth and attribute depth, enabling subsequent prediction of 3D properties such as dimensions and observation angle. In the post-processing phase, we apply a novel depth integration strategy to consolidate RoI depths, taking into account the practical significance of probability distribution. This strategy is outlined in Section~\ref{post_processing}.

\subsubsection{Image Backbone}\label{backbone}
Given an input RGB image $I$ with dimensions $3 \times H \times W$, we utilize a feature backbone $f(\cdot; \Theta)$ to compute the feature map $F$ with dimensions $D \times h \times w$ as Eq.~\eqref{eq:backbone}:
\begin{equation}
	F = f(I;\Theta)
	\label{eq:backbone}
\end{equation}

where $\Theta$ represents all learnable parameters, $D$ denotes the output feature map dimension (e.g., $D=512$), and $h$ and $w$ are determined by the overall subsampling rate $s$ in the backbone (e.g., $s=4$). We employ the DLA-34~\cite{dla} network as our chosen backbone.

\subsubsection{2D Detection Head}\label{2d_detection_head}
Utilizing the output feature map $F$ from the backbone as input, we route it through three detection heads. Each detection head comprises a series of operations: a 2D convolution, Rectified Linear Unit (ReLU) activation function, followed by another 2D convolution. Specifically, the first detection head is responsible for predicting the heatmap $H$ indicating the projected 3D object center. The process of heatmap generation follows the methodology outlined in CenterNet~\cite{centernet}. The second detection head focuses on predicting the offsets $\Delta x$ and $\Delta y$ between the projected 3D object center and the center of the 2D bounding box. Finally, the third detection head is tasked with predicting the width $w_{2d}$ and height $h_{2d}$ of the 2D bounding box.

	\subsubsection{Multi-Scale Grid Attention Module}\label{msga}
During the training phase, we utilize the ground truth projected 3D object center $(x_{c_{gt}}, y_{c_{gt}})$, predicted offsets $\Delta x$ and $\Delta y$, as well as the predicted width $w_{2d}$ and height $h_{2d}$ of the 2D bounding box to compute the RoI using Eq.~\eqref{eq:RoI_generation}. Each RoI is defined by its top-left and bottom-right boundary points.
\begin{equation}
	\begin{aligned}
		RoI =& (x_{c_{gt}}-w_{2d}/2, y_{c_{gt}}-h_{2d}/2, \\
		&x_{c_{gt}}+w_{2d}/2, y_{c_{gt}}+h_{2d}/2)
		\label{eq:RoI_generation}
	\end{aligned}
\end{equation}

During the inference phase, we select the top 50 positions from the heatmap to represent the predicted projected 3D object center. Subsequently, we combine the predicted offset and 2D dimensions to generate the RoI.
\begin{figure*}
	\centering
	\includegraphics[width=1.0\linewidth]{Fig5.eps}
	\caption{Multi-Scale Grid Attention Module. We use three scale to generate the RoI, the RoI feature is obtained by RoI Align. We use 1D convolutional network to generate the attention map of RoI grid. Then we multiply the attention map and the RoI feature, fed to residual summation for the merged feature. The final feature is integrated by the multi-scale merged feature.} 
	\label{fig:multi-scale-grid-attention}
\end{figure*}

Adjacent background information plays a crucial role in distinguishing foreground instances~\cite{pyramid-rcnn}, particularly when the predicted RoI may not accurately encapsulate the object. Similarly, in point cloud-based 3D object detection, such as Pyramid-RCNN~\cite{pyramid-rcnn}, 3D RoIs encounter analogous challenges. To address this issue, enlarging the RoI is a common strategy. In Pyramid RCNN, the RoI is expanded proportionally based on the original RoI. In 3D dimensions, scale remains constant, implying consistent object dimensions irrespective of distance. In contrast, in the image plane, scale varies with distance. Proportional enlargement may not be optimal, potentially introducing excessive background pixels for nearby objects. Thus, we adopt a fixed number of pixels enlargement strategy for the RoI. Determining the required pixels for RoIs with different sizes and varying accuracies of 2D bounding box prediction is challenging. To overcome this challenge, we propose a Multi-Scale configuration, illustrated in Fig~\ref{fig:multi-scale-grid-attention}. Initially, the original RoI is cropped by the predicted 2D bounding box. We uniformly increase the width and height of the 2D bounding box by 5 and 15 pixels, respectively, to create the enlarged RoI. Subsequently, RoI Align~\cite{mask-rcnn} is applied to generate $7\times 7$ RoI features. A $1\times 1$ convolution and sigmoid function are then applied to generate the attention map for each grid. The RoI feature is multiplied by the attention map, and the result is added to the RoI feature to obtain the merged feature. Finally, the multi-scale merged features are concatenated to form the final feature.

The MSGA module consists of two key components: the multi-scale RoI feature and the Grid Attention mechanism. The advantages of employing the multi-scale RoI feature are elucidated as follows:
\begin{itemize}
	\item Enhanced inclusion of foreground pixels: multi-scale features mitigate errors in 2D bounding box predictions, improving the likelihood of encompassing whole foreground pixels of the predicted object.
	\item Improved background information utilization: by incorporating adjacent background pixels, multi-scale features enable better extraction of information from the background region.
\end{itemize}

The advantages of the Grid Attention mechanism are summarized as follows:
\begin{itemize}
	\item Discriminative pixel attention: grid attention enables the model to discern between background and foreground pixels, prioritizing for more accurate training. This discrimination enhances the model's ability to focus on relevant visual elements.
	\item Importance discrimination among foreground pixels: grid attention further discriminates among foreground pixels, assigning higher importance to specific elements. For instance, identifying features like a vehicle's license plate, which serves as a distinctive identifier, is prioritized.
\end{itemize}

\subsubsection{3D Detection Head}\label{3d_detection_head}

For each set of RoI features, we employ seven distinct detection heads to predict various properties. These include the 3D dimensions offset, denoted as $dim_{3d}$, in comparison to the mean size of each class; the offset $offset_{3d}$ representing the quantization error between the projected 3D object center and the corresponding pixel; the visual depth $d_{vis}$ and its associated uncertainty $\sigma_{vis}$; the attribute depth $d_{att}$ and its corresponding uncertainty $\sigma_{att}$; and the observation angle $\theta$.

The detection heads responsible for predicting $dim_{3d}$, $offset_{3d}$, and $\theta$ consist of a sequence comprising a 2D convolution layer followed by batch normalization, Rectified Linear Unit, and another 2D convolution layer. Conversely, the detection heads tasked with predicting $d_{vis}$, $\sigma_{vis}$, $d_{att}$, and $\sigma_{att}$ are composed of a 2D convolution layer, LeakyReLU activation function~\cite{leaky-relu}, and another 2D convolution layer.

\subsubsection{Keypoint Estimation}
For keypoint estimation, we adopt the approach proposed by Monocon~\cite{monocon}. Specifically, we estimate the keypoint heatmap $H_k \in \R^{9\times w \times h}$ to predict the projected positions of the 8 corners and the center of the 2D bounding box. Estimating $H_k$ involves a series of operations: a 2D convolution, ReLU activation, and another 2D convolution. Additionally, we predict the offsets $(\Delta k2c_x, \Delta k2c_y)$between each keypoint and the projected 3D object's center pixels, as well as the quantization error $(\Delta kx, \Delta ky)$ resulting from rounding off the keypoint positions. These components are determined through a sequence of operations: a 2D convolution, batch normalization, ReLU activation, and another 2D convolution. It's important to note that keypoint estimation is employed solely during training to provide supplementary supervision and is not utilized during inference.

\subsubsection{Loss Functions}
\paragraph{2D Heatmap} Our loss function follows the methodology outlined in CenterNet~\cite{centernet}. To guarantee a bounding box overlap of at least 0.7 IoU with the ground truth, we calculate the radius accordingly. Specifically for $H$, we utilize a modified focal loss, defined as Eq.~\eqref{eq:heatmap loss}:

\begin{equation}
	\begin{split}
		L_{heat}=\frac{-1}N\sum_{H}\begin{cases}(1-H)^\alpha\log(H)&\text{if }\hat{H}=1\\(1-\hat{H})^\beta({H})^\alpha \\ \log(1-{H}) &\text{otherwise}\end{cases}
		\label{eq:heatmap loss}
	\end{split}
\end{equation}

where $\hat{H}$ denotes the target heatmap, with $N$ representing the number of keypoints in the image. For our experiments, we set the hyper-parameters $\alpha$ and $\beta$ for the focal loss to 2 and 4, respectively.

\paragraph{2D Box} 
For the 2D bounding boxes, we predict the offsets between the peak in the 2D heatmap $H$ and the center of the 2D bounding box, denoted as $\Delta x_{2d}$ and $\Delta y_{2d}$, along with the sizes of the bounding box, represented by $h_{2d}$ and $w_{2d}$. These values are determined using the L1 loss, as formulated in Eq.~\eqref{eq:2d box loss}.
\begin{equation}
	L_{box_{2d}} = \sum_{o\in \{\Delta x_{2d}, \Delta y_{2d}, w_{2d}, h_{2d}\}}\lvert o - \hat{o} \rvert
	\label{eq:2d box loss}
\end{equation}
where $o$ denotes the predicted value and $\hat{o}$ denotes the regression target.
\paragraph{3D Box}
For 3D box, we need to regress the offset between the peak in 2D heatmap and the projected 3D object center $\Delta x_{3d}, \Delta y_{3d}$, along with the offset of ground truth dimensions and the averaged dimensions $\Delta l_{3d}, \Delta w_{3d}, \Delta h_{3d}$. They are all calculated by the L1 loss, which is formulated as Eq.~\eqref{eq:3d box loss}.
\begin{equation} 
	L_{box_{3d}} = \sum_{o\in \{\Delta{x_{3d}}, \Delta{y_{3d}}, \Delta l_{3d}, \Delta w_{3d}, \Delta h_{3d}\}}\lvert o - \hat{o} \rvert
	\label{eq:3d box loss}
\end{equation}

\paragraph{Depth}
For depth estimation, we make the assumption that the depth follows the Laplace distribution~\cite{didm3d}, and use Eq.~\eqref{eq:depth loss} to calculate different depth and its uncertainty. 
\begin{equation} 
	L_{depth} = \sum_{*\in \{vis, att, ins\}} \frac{\sqrt{2}}{e^{\frac{u_*}{2}}} \times \lvert d_* - \hat{d_*} \rvert + \frac{u_*}{2}
	\label{eq:depth loss}
\end{equation}

where $d_{vis}$, $d_{att}$, and $d_{ins}$ represent the visual depth, attribute depth, and instance depth, respectively, while $u_{vis}$, $u_{att}$, and $u_{ins}$ denote the uncertainty associated with each depth. The symbol $\hat{d_*}$ denotes the regression target. It's essential to clarify that our network predicts only the visual depth and attribute depth, with the instance depth computed as the sum of these two. Additionally, we calculate the instance depth uncertainty using Eq.~\eqref{eq:instance uncertainty}.
\begin{equation} 
	u_{ins} = log(e^{u_{vis}} + e^{u_{att}})
	\label{eq:instance uncertainty}
\end{equation}
\paragraph{Orientation Angle}
We employ the Multi-bin loss~\cite{geometry3d} for the observation angle. Specifically, we partition $2 \pi$ into 12 bins, and the network generates the classification vector $\theta_{cls}$ to identify the bin in which the angle lies, along with the offset $\theta_{reg}$ between the bin center and the ground truth angle. The cross-entropy loss is utilized for $\theta_{cls}$, as formulated in Eq.~\eqref{eq:angle loss}.
\begin{equation} 
	L_{angle_{cls}} = -\sum_{i=1}^{12} y_{i}\log(p_{i})
	\label{eq:angle loss}
\end{equation}

where $y_i$ as the indicator for the $i$th bin, where it equals 1 if the angle falls within the bin, and 0 otherwise. $p_i$ represents the predicted probability of the $i$th bin. As for $\theta_{reg}$, we employ the L1 loss, as formulated in Eq.~\eqref{eq:angle reg loss}.


\begin{equation} 
	L_{angle_{reg}} = \lvert \theta_{reg} - \hat{\theta_{reg}} \rvert
	\label{eq:angle reg loss}
\end{equation}

where $\hat{\theta_{reg}}$ is the regression target.

\paragraph{Keypoints}
We incorporate an auxiliary keypoint loss inspired by Monocon~\cite{monocon}. To generate the keypoint heatmap $H_k \in \R^{9 \times w \times h}$, we project the eight corners onto the image plane and use the center of 2D bounding box, thus representing nine keypoints. The loss function mirrors that of the 2D heatmap, albeit with a looser IoU threshold of 0.3, as adopted in Monocon~\cite{monocon}. This loss function is detailed in Eq.~\eqref{eq:keypoint heatmap loss}.
\begin{equation}
	\begin{split}
		L_{H_k}=\frac{-1}N\sum_{H_k}\begin{cases}(1-H_k)^\alpha\log(H_k)&\text{if }\hat{H_k}=1\\(1-\hat{H_k})^\beta({H_k})^\alpha \\ \log(1-{H_k}) &\text{otherwise}\end{cases}
		\label{eq:keypoint heatmap loss}
	\end{split}
\end{equation}

In addition to the keypoint heatmap, we also predict the offset between each keypoint and the projected 3D object center pixel, denoted as $\Delta k2c_x$ and $\Delta k2c_y$, along with the offset between the peak in the keypoint heatmap and the keypoint position, represented as $\Delta kx$ and $\Delta ky$. These offsets are all calculated using the L1 loss, as formulated in Eq.~\eqref{eq:keypoint loss}.
\begin{equation}
	L_{keypoint} = \sum_{i=1}^{9}\sum_{o\in \{\Delta k2c_x, \Delta k2c_y, \Delta kx, \Delta ky\}}\lvert o^i - \hat{o^i} \rvert
	\label{eq:keypoint loss}
\end{equation}

where $o^i$ denotes the prediction of the $i$-th keypoint, while $\hat{o^i}$ signifies the target corresponding to the $i$-th keypoint.

The total loss is computed as the summation of the aforementioned losses, as defined in Eq. \eqref{eq:total loss}.
\begin{equation}
	\begin{aligned}
		L_{total} =& \lambda_{heat}L_{heat} + \lambda_{box_{2d}}L_{box_{2d}} + \lambda_{box_{3d}}L_{box_{3d}}\\ 
		&+ L_{depth} +\lambda_{angle}(L_{angle_{cls}} + L_{angle_{reg}})\\ 
		&+ \lambda_{H_k}L_{H_k} + \lambda_{keypoint}L_{keypoint}
		\label{eq:total loss}
	\end{aligned}
\end{equation}

where $\lambda_*$ is the weight of each loss, is dynamically adjusted through hierarchical task learning, as proposed by GUPNet~\cite{gupnet}. Details of this adaptation process will be elucidated in Section~\ref{imple}.



\begin{figure}[!t]
	\centering
	\includegraphics[width=1.0\linewidth]{Fig6.eps}
	\caption{Laplace Probability Density Function. Left distribution means $\mu=10, b=0.5$. Middle distribution means $\mu=10, b=1$. Right distribution means $\mu=10, b=2$.}
	\label{fig:laplace distribution}
\end{figure}
\subsection{Probabilistic Post Processing}\label{post_processing}
Previous studies have assumed that depth adheres to the Laplace distribution. The Probability Density Function (PDF) of the Laplace distribution is defined as shown in Eq. \eqref{eq:laplace pdf}. 
\begin{equation}
	f(x|\mu,b)=\frac1{2b}\exp\left(-\frac{|x-\mu|}b\right)
	\label{eq:laplace pdf}
\end{equation}

where $\mu$ represents the positional parameter, and $b$ represents the scale parameter. As illustrated in Fig.~\ref{fig:laplace distribution}, decreasing $b$ sharpens the PDF curve, while increasing it flattens the curve. In our work, the predicted uncertainty is the standard derivation of the Laplace distribution which is directly proportional to $b$. When uncertainty is smaller, the depth distribution becomes more concentrated, whereas larger uncertainty results in a more dispersed depth distribution.

Previous methods~\cite{didm3d, gupnet} have utilized the standard deviation $\sigma$ which is adhere to the predicted uncertainty to assess the accuracy of depth estimation, employing $e^{-\sigma}$ to gauge the confidence level of the estimation. However, the rationale behind importance estimation lacks a solid theoretical foundation. In GUPNet \cite{gupnet}, it is stated that their objective is merely to normalize the value between 0 and 1. In this study, we introduced the Probabilistic Post-Processing (PPP) module to incorporate probabilistic distributions into our method.

Based on fundamental probabilistic principles, the probability of a variable lying within the narrow range $[x-\delta, x+\delta]$ can be computed by calculating the area bounded by the PDF curve and the axis within that range. This computation can be performed using a definite integral or by subtracting the Cumulative Density Function (CDF). The CDF for the Laplace distribution is defined as shown in Eq.~\eqref{eq:laplace cdf}.
\begin{equation}\begin{aligned}
		F(x)& =\int_{-\infty}^xf(u)\mathrm{d}u  \\
		&=\begin{cases}\frac12\exp\left(-\frac{\mu-x}b\right)&\mathrm{if~}x<\mu\\
			\\1-\frac12\exp\left(-\frac{x-\mu}b\right)&\mathrm{if~}x\geq\mu\end{cases} \\
		&=0.5\left[1+\mathrm{sgn}(x-\mu)\left(1-\exp(-|x-\mu|/b)\right)\right]
		\label{eq:laplace cdf}
\end{aligned}\end{equation}
\begin{figure}[!t]
	\centering
	\includegraphics[width=1.0\linewidth]{Fig7.eps}
	\caption{Laplace Cumulative Density Function.}
	\label{fig:laplace distribution cdf}
\end{figure}

where $\mathrm{sgn}(\cdot)$ denotes the sign function, while $F(x)$ represents the probability of a sample lying within the range $(-\infty, x]$. The CDF curve is illustrated in Fig.~\ref{fig:laplace distribution cdf}.

In this work, we use the idea of maximum likelihood function. As above said, we can calculate the probability of the variable locating in the range $[x-\delta, x+\delta]$ by $F(x+\delta)-F(x-\delta)$. And we define the likelihood function as Eq.~\eqref{eq:likelihood function}. 
\begin{equation}
	L(x) = \sum_{1}^{49} F(x+\delta)-F(x-\delta)
	\label{eq:likelihood function}
\end{equation}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=1.0\linewidth]{Fig8.eps}
	\caption{Examples of PPP modules. The red dotted line denotes the ground truth instance depth, while the green dotted line represents the predicted instance depth computed through exponential weighted summation. Our method's predicted depth is indicated by the blue dotted line.}
	\label{fig:ppp}
\end{figure}

We aim to find the optimal depth value that maximizes the likelihood of observing the predictions within a narrow range centered around it. We said we use the idea of maximum likelihood function but not direct this method, because maximum likelihood function is that we know about which distribution the real data follows, and we use the observed data to predict the distribution parameters. However, in our method, we use the predicted distribution to determine the observed position with maximum likelihood. We think our method is more like a multi-model fusion method. In Fig.~\ref{fig:ppp}, we give some examples of the application of our method.

\section{Experiments}\label{sec:exp}
\subsection{Dataset}
\subsubsection{KITTI Dataset}
The KITTI dataset~\cite{kitti} is widely acknowledged as a standard benchmark in the field of 3D object detection. It comprises 7,481 images paired with finely calibrated 3D bounding boxes for training, as well as 7,518 samples designated for testing. We split the training set into two subsets: a training with 3,712 samples and a validation set with 3,769 samples. This division was employed for fine-tuning and optimizing hyperparameters during model development. For the final submissions to the KITTI test server, we train the model on the whole dataset and use the weight of the last epoch.

\subsubsection{Waymo Dataset}
We perform experiments on Waymo dataset~\cite{waymo}, which is a large-scale modern dataset for self-driving. It contains 798 sequences for training and 202 sequences for validation. We sample every $3^{rd}$ frame from the training sequence to form a small training set like CaDDN. The processed training sets have approximately 50k frames.

	\subsection{Implementation Details}\label{imple}
In MSGA module, we use $1\times 1$ convolution and leaky ReLU and $1\times 1$ convolution along with a Sigmoid function to get the attention map. For enlarged RoI, we add no pixels, 5 pixels and 15 pixels, to each side of the RoI. We have test different configuration in the Sec~\ref{sec:exp}. We set $\sigma=0.1$ to refine our post processing process. The value of $\sigma$ is choosen based on the experiments. We use Adam as our optimizer, and set weight decay as 0.00001. We train our model for 200 epochs. At the first 5 epochs, we use the cosine function to transform the learning rate from 0.00001 to 0.001. The learning rate is set to 0.001 in the remaining epochs. And will decay by 0.1 at the 90 and 120 epoch. In the training phase, we remove the frame with no ground truth label for more stable and robust training. For visual depth labeling, we utilize the LRRU method~\cite{lrru} to execute depth completion, thereby generating labeled visual depth data for network training.


\begin{figure}[!t]
	\centering
	\includegraphics[width=1.0\linewidth]{Fig9.eps}
	\caption{Different circumstance of random cropping. When the crop size is smaller than the image size, the image is scaled to fit the crop size, resulting in black edges around the image. Conversely, if the crop size exceeds the image dimensions, the image is scaled to match the crop size, retaining only the portion within the image boundary.}
	\label{fig:affine trans}
\end{figure}
For data augmentation, we use random flip and random crop. After flipping and cropping, affine transformation will be applied on the cropped image to resize the image to the size of $(1280, 384)$ for batch processing. It should be noted that, after cropping and affine transformation, it may cause the processed image having black edge or remove some objects outside the picture, as depicted in Fig.~\ref{fig:affine trans}. For the first condition, for the keypoint of centerpoint that locates in the black space, we calculate their heatmap. For that outside the whole image, we do not calculate their heatmap. But in training phase, the heatmap of black edge do not participate into the loss calculation. For the second condition, if centers of all objects are outside the image, we random sample another item in the dataset.

In the PPP module, optimizing depth determination is framed as an optimization problem, for which we employ the Brent algorithm~\cite{brent} to find the solution. During the training phase, we adopt the hierarchical task learning strategy introduced by GUPNet~\cite{gupnet}. This approach involves training different tasks sequentially. Initially, we focus on training the 2D object detection task, which includes generating 2D heatmaps and bounding boxes. Throughout this phase, $\lambda_{heat}$ and $\lambda_{box_{2d}}$ are both set to 1. Subsequently, the weights $\lambda_{box_{3d}}$ and $\lambda_{angle}$ are adjusted based on the convergence of 2D bounding box predictions. The weights $\lambda_{depth}$, $\lambda_{H_k}$, and $\lambda_{keypoint}$ are determined by the convergence of both 2D bounding box predictions and 3D size predictions. Further details regarding these adjustments can be found in GUPNet~\cite{gupnet}.
\begin{table}[]
	\centering
	\caption{{Car results on KITTI Testing Dataset}}
	\label{tab:kitti_test}
	{%
		\begin{tabular}{ccccc}
			\toprule%?????
			\multirow{2}{*}{{Method}} & \multicolumn{3}{c}{{Car(IoU=0.7)}}                & \multirow{2}{*}{{3D mAP}} \\ \cmidrule{2-4}
			& {Easy}           & {Mod.}           & {Hard}           &                         \\ 
			\midrule%?????
			{MonoDLE~\cite{monodle}}			& {17.23}	& {12.26}	& {10.29}	& {13.29}	\\
			{MonoFlex~\cite{monoflex}}			& {19.94}	& {13.89}   & {12.07}   & {15.30}	\\
			{GUPNet~\cite{gupnet}}            	& {20.11}   & {13.20}   & {11.77}   & {15.03}	\\
			{MonoCon~\cite{monocon}}         	& {22.50}   & {16.46}   & {13.95}   & {17.64}	\\
			{DD3D~\cite{dd3d}}         			& {23.19}   & {16.87}   & {14.36}   & {18.14}	\\
			{MonoDTR~\cite{monodtr}}         	& {21.99}   & {15.39}   & {12.73}   & {16.70}	\\
			{MonoPGC~\cite{monopgc}}         	& {24.68}   & {17.17}   & {14.14}   & {18.66}	\\
			{MonoDETR~\cite{monodetr}} & 25.00 & 16.47 & 13.58 & 18.35
			\\
			{GUPNet++~\cite{gupnet++}}         	& {24.99}   & {16.48}   & {14.58}   & {18.68}	\\
			{Ours}         						& {\textbf{25.43}} 		& {\textbf{17.89}} 		& {\textbf{15.01}} 		& {\textbf{19.44}}   	\\ 
			\bottomrule%?????
		\end{tabular}
	}
\end{table}

\begin{table*}[h]
	\centering
	\caption{{Car results on KITTI validation dataset}}
	\label{tab:kitti_valid}
	\begin{adjustbox}{width=\textwidth}
	{%
		\begin{tabular}{ccccccccccccc}
			\toprule%?????
			\multirow{2}{*}{{Method}} & \multicolumn{3}{c}{{3D@IoU=0.7}} & \multicolumn{3}{c}{{BEV@IoU=0.7}}  & \multicolumn{3}{c}{{3D@IoU=0.5}} & \multicolumn{3}{c}{{BEV@IoU=0.5}}\\ \cmidrule{2-13}
			& {Easy}           & {Mod.}           & {Hard}   & {Easy}           & {Mod.}           & {Hard} & {Easy}           & {Mod.}           & {Hard} & {Easy}           & {Mod.}           & {Hard}                        \\ 
			\midrule%?????
			{MonoDLE~\cite{monodle}}			& {17.45}	& {13.66}	& {11.68}	& 24.97 & 19.33 & 17.01 & 55.41 & 43.42 & 37.81 & 60.73 & 46.87 & 41.89 	\\
			{MonoFlex~\cite{monoflex}}			& {23.64}	& {17.51}   & {14.83}  & - & - & - & - & - & - & - & - & -  	\\
			{GUPNet~\cite{gupnet}}            	& {22.76}   & {16.46}   & {13.72}  & 31.07 & 22.94 & 19.75 & 57.62 & 42.33 & 37.59 & 61.78 & 47.06 & 40.88 	\\
			{MonoCon~\cite{monocon}}         	& {26.33}   & {19.01}   & {15.98}  & - & - & - & - & - & - & - & - & -  	\\
			{MonoDTR~\cite{monodtr}}         	& {24.52}   & {18.57}   & {15.51}  & 33.33 & 25.35 & 21.68 & 64.03 & 47.32 & 42.20 & 69.04 & 52.47 & 45.90 	\\
			{MonoPGC~\cite{monopgc}}         	& {25.67}   & {18.63}   & {15.65}  & 34.06 & 24.26 & 20.78 & - & - & - & - & - & -  	\\
			{MonoDETR~\cite{monodetr}} & 28.84 & 20.61 & 16.38 & - & - & - & - & - & - & - & - & -  	\\
			
			{GUPNet++~\cite{gupnet++}}         	& \textbf{29.03}   & {20.45}   & {17.89}  & 38.82 & 27.95 & 24.96 & 66.66 & 49.65 & 45.23 & 71.55 & 54.00 & 49.34 	\\
			{Ours}         						& {28.93} 		& \textbf{21.07} 		& \textbf{18.13} & \textbf{39.34} 	& \textbf{28.58} & \textbf{25.89} & \textbf{67.44} & \textbf{51.37} & \textbf{46.88} & \textbf{72.34} & \textbf{55.13} & \textbf{50.23}	   	\\ 
			\bottomrule%?????
		\end{tabular}
	}
\end{adjustbox}
\end{table*}


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table*}[h]
	\centering
	\setlength{\tabcolsep}{1mm}
	\caption{Results of Waymo Validation Dataset}
	\label{tab:waymo_valid}

{%
		\begin{tabular}{ccccc}
			\toprule%?????
			\multirow{2}{*}{Methods} & \multicolumn{4}{c}{3D mAP/mAPH}       \\ \cmidrule{2-5}
			& Overall & 0-30m & 30-50m & 50m-$\inf$ \\
			\midrule%?????
			
			\multicolumn{5}{c}{Under Level 1(IoU=0.5)}                       \\
			\midrule%?????
			
			PatchNet~\cite{patchnet}              & {2.92/2.74}  	 & {10.01/9.75} 	 & {1.09/0.96} 		 & {0.23/0.18}	\\
			CaDDN~\cite{caddn}                    & {17.54/17.31}    & \textbf{45.00/44.46}     & {9.24/9.11}       & {0.64/0.62}	\\
			PCT~\cite{pct}                     	  & {4.20/4.15}      & {14.70/14.54}     & {1.78/1.75}       & {0.39/0.39}  \\
			MonoJSG~\cite{monojsg}                & {5.65/5.47}      & {20.86/20.26}     & {3.91/3.79}       & {0.97/0.92}  \\
			SSD-MonoDETR~\cite{ssd-monodetr}      & {11.83/-}    	 & {27.69/-}         & {5.33/-}          & {0.85/-}     \\
			DID-M3D~\cite{didm3d}                 & {20.66/20.47}    & {40.92/40.60}     & {15.63/15.48}     & {5.35/5.24}  \\
			Ours					 			  &	\textbf{22.69/22.33}	 & {43.27/42.89}	 &	\textbf{16.94/16.72} &	\textbf{6.38/6.12}	 			\\
			\midrule%?????
			\multicolumn{5}{c}{Under Level 2(IoU=0.5)}                       \\
			\midrule%?????
			PatchNet~\cite{patchnet}              & {2.42/2.28}      & {10.01/9.73}      & {1.07/0.94}       & {0.22/0.16}  \\
			CaDDN~\cite{caddn}                    & {16.51/16.28}    & \textbf{44.87/44.33}     & {8.99/8.86}       & {0.58/0.55}	\\
			PCT~\cite{pct}                        & {4.03/3.99}      & {14.67/14.51}     & {1.74/1.71}       & {0.36/0.35}  \\
			MonoJSG~\cite{monojsg}                & {5.34/5.17}      & {20.79/20.19}     & {3.79/3.67}       & {0.85/0.82}  \\
			SSD-MonoDETR~\cite{ssd-monodetr}      & {11.34/-}    	 & {27.62/-}     	 & {5.21/-}          & {0.76/-}     \\
			DID-M3D~\cite{didm3d}                 & {19.37/19.19}    & {40.77/40.46}     & {15.18/15.04}     & {4.69/4.59}	\\
			Ours & \textbf{21.47/21.33} & {42.39/42.14} & \textbf{16.53/16.37} &\textbf{5.12/5.01}			 								 									\\
			\bottomrule%?????
		\end{tabular}%
	}
\end{table*}
\subsection{Experiment Results}
\subsubsection{KITTI Test Dataset}
We perform experiments on the KITTI test dataset and present the comparison between our method and state-of-the-art approaches in Table~\ref{tab:kitti_test}, with the best results highlighted in bold. The results demonstrate that our method surpasses others.

\subsubsection{KITTI Validation Dataset}

Experiments were conducted on the KITTI validation dataset, and the results are presented in Table~\ref{tab:kitti_valid}, with superior outcomes highlighted in bold. Our method demonstrates superior performance across most metrics, except for the Easy subset with 0.7 IoU threshold. However, on the test dataset, our model outperforms GUPNet++ across all metrics. This suggests that GUPNet++ may suffer from slight overfitting, resulting in a smaller performance gap between our method and theirs on the validation dataset.

\subsubsection{Waymo Validation Dataset}

Experiments are conducted on the Waymo validation dataset, and the comparison between our method and other approaches is presented in Table~\ref{tab:waymo_valid}, with the best results highlighted in bold. Since many other methods only conduct experiments on a subset of the Waymo Dataset, we adopt the same strategy for fair comparison. The results shows that our methods outperform other methods except CaDDN in 0-30m. We think that this is because CaDDN use the depth classification for depth estimation, and this type of method will may more attention to the near objects, and our methods treat the objects in different distances more balanced.

\begin{table}[]
	\centering
	\setlength{\tabcolsep}{1mm}
	\caption{Effects of different components}
	\label{tab:diff_com}
	{%
		\begin{tabular}{cccccccc}
			
			\hline
			\multirow{2}{*}{{No.}}  & \multirow{2}{*}{MS}       & \multirow{2}{*}{GA}      & \multirow{2}{*}{PPP}       & \multicolumn{3}{c}{{Car (IoU=0.7)}} & \multirow{2}{*}{3D mAP} \\ \cmidrule{5-7}
			& &                           &                           & Easy  & Mod.  & Hard  &       \\ \hline
			{(a)}&   			&              &              	& 26.46 & 18.43 & 15.32 &  20.07	\\
			{(b)}& \checkmark   &              &              	& 27.56 & 19.32 & 15.98 &  20.95	\\
			{(c)}& 				& 	\checkmark &				& 27.23 & 19.01 & 16.47 &  20.90 	\\
			{(d)}& 				&              & \checkmark 	& 27.34 & 19.45 & 16.35 &  21.05	\\
			{(e)}& {\checkmark} &{\checkmark}  & 			 	& 28.12 & 20.31 & 17.12 &  21.85 	\\
			{(f)}&           	&{\checkmark}  & {\checkmark} 	& 28.33 & 20.45 & 17.56 &  22.11 	\\
			{(g)}&{\checkmark}  &              & {\checkmark} 	& 28.42 & 20.44 & 17.43 &  22.10	\\
			{(h)}&\checkmark    & \checkmark   & \checkmark     & \textbf{28.93} & \textbf{21.07} 		&\textbf{18.13} & \textbf{22.71}	\\ \hline
		\end{tabular}%
	}
\end{table}

\begin{table*}[]
	\centering
		\setlength{\tabcolsep}{1mm}
	\caption{Effects of Multi-Scale Methods}
	\label{tab:multi scale}
	{%
		\begin{tabular}{cccccccc}
			\hline
			\multirow{2}{*}{{FT}}  & \multirow{2}{*}{CD} & \multirow{2}{*}{Scale} & \multicolumn{3}{c}{{Car (IoU=0.7)}} & \multirow{2}{*}{3D mAP} \\ \cmidrule{4-6} & & & Easy  & Mod.  & Hard  &       \\ \hline
			5		&5		&-						& 28.67 & 20.88 & \textbf{18.23} & 22.59 	\\
			5		&10		&-						& {28.93} & \textbf{21.07} 		&{18.13} & \textbf{22.71}	\\
			5		&15		&-						& 28.88 & 20.78 & 17.69 & 22.45 	\\
			10		&5		&-						& 28.91 & 20.83 & 17.54 & 22.43 	\\
			10		&10		&-						& \textbf{29.01} & 20.75 & 17.49 & 22.42 	\\
			10		&15		&-						& 28.74 & 20.69 & 16.58 & 20.00 	\\
			15		&5		&-						& 28.63 & 20.83 & 16.79 & 22.08 	\\
			15		&10		&-						& 28.66 & 20.66 & 16.54 & 21.95 	\\
			15		&15		&-						& 28.74 & 20.88 & 16.32 & 21.98 	\\ 
			-		&-		&(1.0, 1.05, 1.1)		& 25.42 & 17.56 & 14.49 & 19.16 	\\ 
			-		&-		&(1.1, 1.2, 1.3)		& 24.95 & 16.69 & 13.78 & 18.47 	\\ \hline
		\end{tabular}%
	}
\end{table*}

\begin{table}[]
	\centering
	\caption{Effects of Grid Attention Activation Function}
	\label{tab:grid attention activate function}
	{%
		\begin{tabular}{ccccc}
			\hline
			\multirow{2}{*}{{Method}} & \multicolumn{3}{c}{{Car (IoU=0.7)}} & \multirow{2}{*}{3D mAP} \\ \cmidrule{2-4}
			& Easy  & Mod.  & Hard  &       \\ \hline
			Sigmoid 		& {28.93} & \textbf{21.07} 		&\textbf{18.13} & \textbf{22.71} 	\\
			Hard Sigmoid 	& 28.74 & 21.05 & 17.74 & 22.51 	\\
			Tanh 			& \textbf{29.03} & 20.94 & 17.99 & {22.65}  	\\
			Softmax			& 27.93 & 20.66 & 17.32 & {21.97}  	\\ \hline
		\end{tabular}%
	}
\end{table}

\begin{table}[]
	\centering
	\caption{Effects of Probabilistic Post Processing}
	\label{tab:ppp module}
	{%
		\begin{tabular}{ccccc}
			\hline
			\multirow{2}{*}{{Method}} & \multicolumn{3}{c}{{Car (IoU=0.7)}} & \multirow{2}{*}{3D mAP} \\ \cmidrule{2-4}
			& Easy  & Mod.  & Hard  &       \\ \hline
			$\delta$=0.05 		& {27.95} & {20.13} 		&{16.54} & {21.54}  	\\
			$\delta$=0.1 		& \textbf{28.93} & \textbf{21.07} 		&\textbf{18.13} & \textbf{22.71}  	\\
			$\delta$=0.2 		& 28.66 & 20.73 & 17.63 & 22.34 	\\
			$\delta$=0.3  		& 28.65 & 20.76 & 17.79 & 22.40  	\\
			$\delta$=0.4 		& 28.03 & 20.24 & 17.02 & 21.76  	\\ \hline
		\end{tabular}%
	}
\end{table}

\subsection{Ablation Studies}
\subsubsection{Influence of different components}
We evaluated the MSGA and PPP modules as presented in Table~\ref{tab:diff_com}. Our baseline model, without these modules, serves as a reference point. Results from experiments (b), (c) and (d) demonstrate that incorporating either module individually enhances performance. Moreover, for combinations involving both modules, such as (e), (f) and (g), those with PPP exhibit superior performance compared to those without. This suggests that probabilistic information plays a crucial role in network learning, particularly for models with probabilistic assumptions. Experiment (h) indicates that the model incorporating all modules achieves the highest performance, highlighting the effectiveness of our proposed modules in directing attention to RoI grids and leveraging probabilistic cues more effectively.


\subsubsection{Influence of Multi-Scale Methods}
We tested the impact of enlarging the RoI on performance. Initially, we uniformly increased both the width and height by the same number of pixels, and also explored proportionally enlarging the RoI. Our findings revealed a significant performance drop when enlarging the bounding box by factors of 1.0, 1.05, and 1.1. Further proportional increases exacerbated this decline due to scale variance in the image plane. Conversely, when adopting a strategy of enlarging the RoI by a fixed number of pixels, we devised a scheme to generate additional pixels in a multi-scale fashion, forming an arithmetic series based on the First Term (FT) and the Common Difference (CD). The experiment is depicted in Table~\ref{tab:multi scale}. Increasing the FT and CD led to deteriorating performance in hard partitions. This is likely because objects in hard partitions typically have small 2D bounding boxes, causing background pixels to dominate the learning process when more pixels are added. Conversely, changes in FT and CD had less impact on easy and moderate partitions, consistent with our assumption that objects in these partitions have larger 2D bounding boxes, making minor pixel adjustments less influential. Proportionally enlarging the bounding box resulted in notably worse performance compared to our methods, attributed to scale variance in the image plane. Even in hard partitions where bounding boxes are small, performance was inferior to simply adding fixed pixels. This discrepancy may stem from the network's confusion regarding background and foreground pixels due to varying bounding box sizes and the corresponding distribution of background and foreground pixels within the enlarged bounding box. The prevalence of large objects outweighs that of small objects, indicating an imbalance in object sizes. Proportionally enlarging RoIs exacerbates this issue, resulting in a significant increase in background pixels. Consequently, the network might inadvertently learn the relative proportion of background to foreground pixels. This can lead to confusion during the processing of small objects, hindering the model's learning process.

\subsubsection{Influence of Grid Attention Activation Function}
To scale the output of the attention map to the range $[0, 1]$, we evaluate several activation functions for comparison, as detailed in Table~\ref{tab:grid attention activate function}. Sigmoid, Hard Sigmoid~\cite{hard-sigmoid}, and Softmax are directly applied to the attention map. However, since the Tanh function maps values to the range $[-1, 1]$, we employ the transformation outlined in Eq.~\eqref{eq:tanh} to map these values to $[0, 1]$.
\begin{equation}
	\begin{aligned}
		att = (1+tanh(x))/2
		\label{eq:tanh}
	\end{aligned}
\end{equation}

where $att$ is the attention map, $x$ is the output of the network.

The experimental results are presented in Table~\ref{tab:grid attention activate function}. Sigmoid, hard sigmoid, and tanh functions exhibit comparable performance, while the softmax function performs notably worse. This discrepancy can be attributed to the softmax function's tendency to assign high attention to one grid and low attention to others. However, since the RoI is not determined solely by one grid, this approach proves less effective.

\subsubsection{Influence of Probabilistic Posting Processing}
We selected the hyper-parameters $\delta$ in the Probabilistic Post-Processing (PPP) module to optimize its performance. If $\delta$ is too small, the model tends to focus solely on predictions with the lowest uncertainty. Conversely, if $\delta$ is too large, the differences between positions become less discernible. The experimental findings are detailed in Table~\ref{tab:ppp module}. Setting $\delta$ to values such as 0.05 or 0.4 leads to a drop in performance. Consequently, we determined that $\delta=0.1$ yields the best results based on our experiments.

\section{Conclusion}\label{conclusion}
In this paper, we delve into the challenges of RoI-based monocular 3D object detection. We identify two main issues: errors in 2D bounding box prediction and overlooked variations in RoI grid weights, hindering effective network learning. To address these issues, we introduce the MSGA module, which allocates grid attention intelligently and integrates multi-scale features to mitigate prediction errors. Additionally, acknowledging the probabilistic assumptions prevalent in existing methods, we propose the PPP module to better leverage probabilistic cues. Both modules significantly enhance baseline performance. However, our method employs an independent depth completion network to generate ground truth visual depth, which, although not entirely accurate, may introduce noise into the training data. Hence, denoising is a future avenue of research to enhance the robustness of our network.

\bmhead{Acknowledgements}
This study is supported in part by the National Natural Science Foundation of China (62272194, 62172186), and in part by the Science and Technology Development Plan Project of Jilin Province (20220101101JC). 

\section*{Statements and Declarations}
\begin{itemize}
	\item Funding. This study is supported in part by the National Natural Science Foundation of China (62272194, 62172186), and in part by the Science and Technology Development Plan Project of Jilin Province (20220101101JC). 
	\item Conflict of interest/Competing interests. Not applicable.
	\item Ethics approval and consent to participate. Not applicable.
	\item Consent for publication. Not applicable.
	\item Data availability. The dataset can be obtained on \href{https://www.cvlibs.net/datasets/kitti/}{KITTI Datset} and \href{https://waymo.com/open/}{Waymo Dataset}.
	\item Materials availability. Not applicable. 
	\item Code availability. The source code is available upon request by contacting the corresponding author.
	\item Author contributions: Investigation was conducted by Tingyu Zhang and Xinyu Yang; Methodology was introduced by Tingyu Zhang; Zhigang Liang and Yanzhao Yang performed original draft preparation; Review and editing were carried out by Tingyu Zhang, Xinyu Yang, Zhigang Liang, Yanzhao Yang, and Jian Wang. All authors have read and approved the final version of the manuscript.
\end{itemize}

\bibliography{sn-bibliography.bib}




\end{document}
