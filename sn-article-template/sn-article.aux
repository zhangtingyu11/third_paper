\relax 
\providecommand\hyper@newdestlabel[2]{}
\bibstyle{sn-basic}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{segment-fusion,srif}
\providecommand \oddpage@label [2]{}
\Newlabel{1}{1}
\Newlabel{2}{2}
\citation{teng}
\citation{gupnet,didm3d,neurocs}
\citation{neurocs}
\citation{gupnet,didm3d}
\citation{fast-rcnn}
\citation{roi-warp}
\citation{mask-rcnn}
\citation{didm3d}
\citation{kitti}
\citation{didm3d}
\citation{gupnet,didm3d}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{sec1}{{1}{2}{Introduction}{section.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Instance and its depth residual. The depth residual of an instance is computed by taking the absolute difference between the predicted depth generated by the depth completion network and the corresponding ground truth. The left column shows the raw image of the instance, while the right column illustrates the depth residual. Lighter colors in the latter column indicate higher depth residuals.}}{3}{figure.1}\protected@file@percent }
\newlabel{fig:depth_residual}{{1}{3}{Instance and its depth residual. The depth residual of an instance is computed by taking the absolute difference between the predicted depth generated by the depth completion network and the corresponding ground truth. The left column shows the raw image of the instance, while the right column illustrates the depth residual. Lighter colors in the latter column indicate higher depth residuals}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visual depth offset. The visual depth of the central grid within the RoI is considered the baseline, from which the offset to other grids is computed. This absolute offset is subsequently labeled for each grid.}}{4}{figure.2}\protected@file@percent }
\newlabel{fig:visual_depth_offset}{{2}{4}{Visual depth offset. The visual depth of the central grid within the RoI is considered the baseline, from which the offset to other grids is computed. This absolute offset is subsequently labeled for each grid}{figure.2}{}}
\citation{geometry3d}
\citation{pgd}
\citation{monopair}
\citation{ddmp-3d}
\citation{m3d-rpn,d4lcn,monodtr}
\citation{oft}
\citation{imvoxelnet}
\citation{caddn}
\citation{mononerd}
\citation{color-embeded,mono-plidar,patchnet}
\citation{dd3d}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Decoupled depth. The instance depth is defined as the distance between the object center to the camera plane. And the visual depth is distance of object surface to the camera plane. The attribute depth is obtained by the minus of instance depth and visual depth.}}{5}{figure.3}\protected@file@percent }
\newlabel{fig:decoupled depth}{{3}{5}{Decoupled depth. The instance depth is defined as the distance between the object center to the camera plane. And the visual depth is distance of object surface to the camera plane. The attribute depth is obtained by the minus of instance depth and visual depth}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{5}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Monocular 3D Object Detection without RoI}{5}{subsection.2.1}\protected@file@percent }
\citation{monodle}
\citation{polygon}
\citation{rtm3d}
\citation{smoke}
\citation{monodde}
\citation{monocon}
\citation{m3dssd}
\citation{monopgc}
\citation{cie}
\citation{ssd-monodetr}
\citation{monoflex}
\citation{neurocs,monoxiver}
\citation{roi10d,didm3d,gupnet,gupnet++}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Monocular 3D Object Detection with RoI}{6}{subsection.2.2}\protected@file@percent }
\citation{geometry3d}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Overview of our model. Initially, the input image undergoes processing through the backbone network to extract image features. These features are subsequently utilized for 2D detection, keypoint estimation, and 3D heatmap generation. Leveraging the results of 2D detection, the Region of Interest (RoI) features are directed to the Multi-Scale Grid Attention (MSGA) module for refinement. The enhanced RoI features are then employed to predict the 3D offset, observation angle, and depth. The depth estimation undergoes optimization via the Probabilistic Post-Processing (PPP) module. Finally, the results are decoded by both 3D properties and optimized depth.}}{7}{figure.4}\protected@file@percent }
\newlabel{fig:overview}{{4}{7}{Overview of our model. Initially, the input image undergoes processing through the backbone network to extract image features. These features are subsequently utilized for 2D detection, keypoint estimation, and 3D heatmap generation. Leveraging the results of 2D detection, the Region of Interest (RoI) features are directed to the Multi-Scale Grid Attention (MSGA) module for refinement. The enhanced RoI features are then employed to predict the 3D offset, observation angle, and depth. The depth estimation undergoes optimization via the Probabilistic Post-Processing (PPP) module. Finally, the results are decoded by both 3D properties and optimized depth}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{7}{section.3}\protected@file@percent }
\newlabel{sec:method}{{3}{7}{Method}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Problem Definition}{7}{subsection.3.1}\protected@file@percent }
\newlabel{eq:projection_matrix}{{1}{7}{Problem Definition}{equation.3.1}{}}
\citation{monocon}
\citation{didm3d}
\citation{dla}
\citation{centernet}
\newlabel{eq:projection}{{2}{8}{Problem Definition}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Overview}{8}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Image Backbone}{8}{subsubsection.3.2.1}\protected@file@percent }
\newlabel{backbone}{{3.2.1}{8}{Image Backbone}{subsubsection.3.2.1}{}}
\newlabel{eq:backbone}{{3}{8}{Image Backbone}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}2D Detection Head}{8}{subsubsection.3.2.2}\protected@file@percent }
\newlabel{2d_detection_head}{{3.2.2}{8}{2D Detection Head}{subsubsection.3.2.2}{}}
\citation{pyramid-rcnn}
\citation{pyramid-rcnn}
\citation{mask-rcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Multi-Scale Grid Attention Module. We use three scale to generate the RoI, the RoI feature is obtained by RoI Align. We use 1D convolutional network to generate the attention map of RoI grid. Then we multiply the attention map and the RoI feature, fed to residual summation for the merged feature. The final feature is integrated by the multi-scale merged feature.}}{9}{figure.5}\protected@file@percent }
\newlabel{fig:multi-scale-grid-attention}{{5}{9}{Multi-Scale Grid Attention Module. We use three scale to generate the RoI, the RoI feature is obtained by RoI Align. We use 1D convolutional network to generate the attention map of RoI grid. Then we multiply the attention map and the RoI feature, fed to residual summation for the merged feature. The final feature is integrated by the multi-scale merged feature}{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Multi-Scale Grid Attention Module}{9}{subsubsection.3.2.3}\protected@file@percent }
\newlabel{msga}{{3.2.3}{9}{Multi-Scale Grid Attention Module}{subsubsection.3.2.3}{}}
\newlabel{eq:RoI_generation}{{4}{9}{Multi-Scale Grid Attention Module}{equation.3.4}{}}
\citation{leaky-relu}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}3D Detection Head}{10}{subsubsection.3.2.4}\protected@file@percent }
\newlabel{3d_detection_head}{{3.2.4}{10}{3D Detection Head}{subsubsection.3.2.4}{}}
\citation{monocon}
\citation{centernet}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.5}Keypoint Estimation}{11}{subsubsection.3.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.6}Loss Functions}{11}{subsubsection.3.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2D Heatmap}{11}{section*.2}\protected@file@percent }
\newlabel{eq:heatmap loss}{{5}{11}{2D Heatmap}{equation.3.5}{}}
\@writefile{toc}{\contentsline {paragraph}{2D Box}{11}{section*.3}\protected@file@percent }
\newlabel{eq:2d box loss}{{6}{11}{2D Box}{equation.3.6}{}}
\citation{didm3d}
\citation{geometry3d}
\citation{monocon}
\citation{monocon}
\@writefile{toc}{\contentsline {paragraph}{3D Box}{12}{section*.4}\protected@file@percent }
\newlabel{eq:3d box loss}{{7}{12}{3D Box}{equation.3.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Depth}{12}{section*.5}\protected@file@percent }
\newlabel{eq:depth loss}{{8}{12}{Depth}{equation.3.8}{}}
\newlabel{eq:instance uncertainty}{{9}{12}{Depth}{equation.3.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Orientation Angle}{12}{section*.6}\protected@file@percent }
\newlabel{eq:angle loss}{{10}{12}{Orientation Angle}{equation.3.10}{}}
\newlabel{eq:angle reg loss}{{11}{12}{Orientation Angle}{equation.3.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Keypoints}{12}{section*.7}\protected@file@percent }
\citation{gupnet}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Laplace Probability Density Function. Left distribution means $\mu =10, b=0.5$. Middle distribution means $\mu =10, b=1$. Right distribution means $\mu =10, b=2$.}}{13}{figure.6}\protected@file@percent }
\newlabel{fig:laplace distribution}{{6}{13}{Laplace Probability Density Function. Left distribution means $\mu =10, b=0.5$. Middle distribution means $\mu =10, b=1$. Right distribution means $\mu =10, b=2$}{figure.6}{}}
\newlabel{eq:keypoint heatmap loss}{{12}{13}{Keypoints}{equation.3.12}{}}
\newlabel{eq:keypoint loss}{{13}{13}{Keypoints}{equation.3.13}{}}
\newlabel{eq:total loss}{{14}{13}{Keypoints}{equation.3.14}{}}
\citation{didm3d,gupnet}
\citation{gupnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Probabilistic Post Processing}{14}{subsection.3.3}\protected@file@percent }
\newlabel{post_processing}{{3.3}{14}{Probabilistic Post Processing}{subsection.3.3}{}}
\newlabel{eq:laplace pdf}{{15}{14}{Probabilistic Post Processing}{equation.3.15}{}}
\newlabel{eq:laplace cdf}{{16}{14}{Probabilistic Post Processing}{equation.3.16}{}}
\newlabel{eq:likelihood function}{{17}{14}{Probabilistic Post Processing}{equation.3.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Laplace Cumulative Density Function.}}{15}{figure.7}\protected@file@percent }
\newlabel{fig:laplace distribution cdf}{{7}{15}{Laplace Cumulative Density Function}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Examples of PPP modules. The red dotted line denotes the ground truth instance depth, while the green dotted line represents the predicted instance depth computed through exponential weighted summation. Our method's predicted depth is indicated by the blue dotted line.}}{16}{figure.8}\protected@file@percent }
\newlabel{fig:ppp}{{8}{16}{Examples of PPP modules. The red dotted line denotes the ground truth instance depth, while the green dotted line represents the predicted instance depth computed through exponential weighted summation. Our method's predicted depth is indicated by the blue dotted line}{figure.8}{}}
\citation{kitti}
\citation{waymo}
\citation{lrru}
\citation{brent}
\citation{gupnet}
\citation{gupnet}
\citation{monodle}
\citation{monoflex}
\citation{gupnet}
\citation{monocon}
\citation{dd3d}
\citation{monodtr}
\citation{monopgc}
\citation{monodetr}
\citation{gupnet++}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{17}{section.4}\protected@file@percent }
\newlabel{sec:exp}{{4}{17}{Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Dataset}{17}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}KITTI Dataset}{17}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Waymo Dataset}{17}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Implementation Details}{17}{subsection.4.2}\protected@file@percent }
\newlabel{imple}{{4.2}{17}{Implementation Details}{subsection.4.2}{}}
\citation{monodle}
\citation{monoflex}
\citation{gupnet}
\citation{monocon}
\citation{monodtr}
\citation{monopgc}
\citation{monodetr}
\citation{gupnet++}
\citation{patchnet}
\citation{caddn}
\citation{pct}
\citation{monojsg}
\citation{ssd-monodetr}
\citation{didm3d}
\citation{patchnet}
\citation{caddn}
\citation{pct}
\citation{monojsg}
\citation{ssd-monodetr}
\citation{didm3d}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Different circumstance of random cropping. When the crop size is smaller than the image size, the image is scaled to fit the crop size, resulting in black edges around the image. Conversely, if the crop size exceeds the image dimensions, the image is scaled to match the crop size, retaining only the portion within the image boundary.}}{18}{figure.9}\protected@file@percent }
\newlabel{fig:affine trans}{{9}{18}{Different circumstance of random cropping. When the crop size is smaller than the image size, the image is scaled to fit the crop size, resulting in black edges around the image. Conversely, if the crop size exceeds the image dimensions, the image is scaled to match the crop size, retaining only the portion within the image boundary}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Experiment Results}{18}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}KITTI Test Dataset}{18}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces {Car results on KITTI Testing Dataset}}}{19}{table.1}\protected@file@percent }
\newlabel{tab:kitti_test}{{1}{19}{Car results on KITTI Testing Dataset}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces {Car results on KITTI validation dataset}}}{19}{table.2}\protected@file@percent }
\newlabel{tab:kitti_valid}{{2}{19}{Car results on KITTI validation dataset}{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}KITTI Validation Dataset}{19}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Waymo Validation Dataset}{19}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Results of Waymo Validation Dataset}}{20}{table.3}\protected@file@percent }
\newlabel{tab:waymo_valid}{{3}{20}{Results of Waymo Validation Dataset}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Effects of different components}}{20}{table.4}\protected@file@percent }
\newlabel{tab:diff_com}{{4}{20}{Effects of different components}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Ablation Studies}{20}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Influence of different components}{20}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Effects of Multi-Scale Methods}}{21}{table.5}\protected@file@percent }
\newlabel{tab:multi scale}{{5}{21}{Effects of Multi-Scale Methods}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Effects of Grid Attention Activation Function}}{21}{table.6}\protected@file@percent }
\newlabel{tab:grid attention activate function}{{6}{21}{Effects of Grid Attention Activation Function}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Effects of Probabilistic Post Processing}}{21}{table.7}\protected@file@percent }
\newlabel{tab:ppp module}{{7}{21}{Effects of Probabilistic Post Processing}{table.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Influence of Multi-Scale Methods}{21}{subsubsection.4.4.2}\protected@file@percent }
\citation{hard-sigmoid}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Influence of Grid Attention Activation Function}{22}{subsubsection.4.4.3}\protected@file@percent }
\newlabel{eq:tanh}{{18}{22}{Influence of Grid Attention Activation Function}{equation.4.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.4}Influence of Probabilistic Posting Processing}{22}{subsubsection.4.4.4}\protected@file@percent }
\bibdata{sn-bibliography.bib}
\bibcite{m3d-rpn}{{1}{2019}{{Brazil and Liu}}{{}}}
\bibcite{brent}{{2}{2013}{{Brent}}{{}}}
\bibcite{polygon}{{3}{2020}{{Cai et~al}}{{Cai, Li, Jiao, Li, Zeng, and Wang}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{23}{section.5}\protected@file@percent }
\newlabel{conclusion}{{5}{23}{Conclusion}{section.5}{}}
\@writefile{toc}{\contentsline {subparagraph}{Acknowledgements}{23}{section*.8}\protected@file@percent }
\bibcite{monopair}{{4}{2020}{{Chen et~al}}{{Chen, Tai, Sun, and Li}}}
\bibcite{hard-sigmoid}{{5}{2015}{{Courbariaux et~al}}{{Courbariaux, Bengio, and David}}}
\bibcite{roi-warp}{{6}{2016}{{Dai et~al}}{{Dai, He, and Sun}}}
\bibcite{d4lcn}{{7}{2020}{{Ding et~al}}{{Ding, Huo, Yi, Wang, Shi, Lu, and Luo}}}
\bibcite{kitti}{{8}{2012}{{Geiger et~al}}{{Geiger, Lenz, and Urtasun}}}
\bibcite{fast-rcnn}{{9}{2015}{{Girshick}}{{}}}
\bibcite{mask-rcnn}{{10}{2017}{{He et~al}}{{He, Gkioxari, Doll{\'a}r, and Girshick}}}
\bibcite{ssd-monodetr}{{11}{2023}{{He et~al}}{{He, Yang, Yang, Lin, Fu, Wang, Yuan, and Li}}}
\bibcite{monodtr}{{12}{2022}{{Huang et~al}}{{Huang, Wu, Su, and Hsu}}}
\bibcite{rtm3d}{{13}{2020}{{Li et~al}}{{Li, Zhao, Liu, and Cao}}}
\bibcite{srif}{{14}{2023}{{Li and Kong}}{{}}}
\bibcite{monodde}{{15}{2022}{{Li et~al}}{{Li, Qu, Zhou, Liu, Wang, and Jiang}}}
\bibcite{monojsg}{{16}{2022}{{Lian et~al}}{{Lian, Li, and Chen}}}
\bibcite{monocon}{{17}{2022}{{Liu et~al}}{{Liu, Xue, and Wu}}}
\bibcite{monoxiver}{{18}{2023}{{Liu et~al}}{{Liu, Zheng, Cheng, Xue, Qi, and Wu}}}
\bibcite{smoke}{{19}{2020}{{Liu et~al}}{{Liu, Wu, and T{\'o}th}}}
\bibcite{gupnet}{{20}{2021}{{Lu et~al}}{{Lu, Ma, Yang, Zhang, Liu, Chu, Yan, and Ouyang}}}
\bibcite{gupnet++}{{21}{2023}{{Lu et~al}}{{Lu, Ma, Yang, Zhang, Liu, Chu, He, Li, and Ouyang}}}
\bibcite{m3dssd}{{22}{2021}{{Luo et~al}}{{Luo, Dai, Shao, and Ding}}}
\bibcite{color-embeded}{{23}{2019}{{Ma et~al}}{{Ma, Wang, Li, Zhang, Ouyang, and Fan}}}
\bibcite{patchnet}{{24}{2020}{{Ma et~al}}{{Ma, Liu, Xia, Zhang, Zeng, and Ouyang}}}
\bibcite{monodle}{{25}{2021}{{Ma et~al}}{{Ma, Zhang, Xu, Zhou, Yi, Li, and Ouyang}}}
\bibcite{leaky-relu}{{26}{2013}{{Maas et~al}}{{Maas, Hannun, Ng et~al}}}
\bibcite{roi10d}{{27}{2019}{{Manhardt et~al}}{{Manhardt, Kehl, and Gaidon}}}
\bibcite{pyramid-rcnn}{{28}{2021}{{Mao et~al}}{{Mao, Niu, Bai, Liang, Xu, and Xu}}}
\bibcite{neurocs}{{29}{2023}{{Min et~al}}{{Min, Zhuang, Schulter, Liu, Dunn, and Chandraker}}}
\bibcite{geometry3d}{{30}{2017}{{Mousavian et~al}}{{Mousavian, Anguelov, Flynn, and Kosecka}}}
\bibcite{dd3d}{{31}{2021}{{Park et~al}}{{Park, Ambrus, Guizilini, Li, and Gaidon}}}
\bibcite{didm3d}{{32}{2022}{{Peng et~al}}{{Peng, Wu, Yang, Liu, and Cai}}}
\bibcite{caddn}{{33}{2021}{{Reading et~al}}{{Reading, Harakeh, Chae, and Waslander}}}
\bibcite{oft}{{34}{2018}{{Roddick et~al}}{{Roddick, Kendall, and Cipolla}}}
\bibcite{imvoxelnet}{{35}{2022}{{Rukhovich et~al}}{{Rukhovich, Vorontsova, and Konushin}}}
\bibcite{waymo}{{36}{2020}{{Sun et~al}}{{Sun, Kretzschmar, Dotiwalla, Chouard, Patnaik, Tsui, Guo, Zhou, Chai, Caine et~al}}}
\bibcite{segment-fusion}{{37}{2023}{{Tao et~al}}{{Tao, Bian, Wang, Li, Gao, Zhang, Zheng, and Zhu}}}
\bibcite{teng}{{38}{2024}{{Teng et~al}}{{Teng, Li, Li, Hu, Li, Ai, and Chen}}}
\bibcite{ddmp-3d}{{39}{2021{a}}{{Wang et~al}}{{Wang, Du, Ye, Fu, Guo, Xue, Feng, and Zhang}}}
\bibcite{pct}{{40}{2021{b}}{{Wang et~al}}{{Wang, Zhang, Zhu, Zhang, He, Li, and Xue}}}
\bibcite{pgd}{{41}{2022}{{Wang et~al}}{{Wang, Xinge, Pang, and Lin}}}
\bibcite{lrru}{{42}{2023}{{Wang et~al}}{{Wang, Li, Zhang, Liu, Gao, and Dai}}}
\bibcite{mono-plidar}{{43}{2019}{{Weng and Kitani}}{{}}}
\bibcite{monopgc}{{44}{2023}{{Wu et~al}}{{Wu, Gan, Wang, Chen, and Pu}}}
\bibcite{mononerd}{{45}{2023}{{Xu et~al}}{{Xu, Peng, Cheng, Li, Qian, Li, Wang, and Cai}}}
\bibcite{cie}{{46}{2022}{{Ye et~al}}{{Ye, Jiang, Zhen, and Du}}}
\bibcite{dla}{{47}{2018}{{Yu et~al}}{{Yu, Wang, Shelhamer, and Darrell}}}
\bibcite{monodetr}{{48}{2023}{{Zhang et~al}}{{Zhang, Qiu, Wang, Guo, Cui, Qiao, Li, and Gao}}}
\bibcite{monoflex}{{49}{2021}{{Zhang et~al}}{{Zhang, Lu, and Zhou}}}
\bibcite{centernet}{{50}{2019}{{Zhou et~al}}{{Zhou, Wang, and Kr{\"a}henb{\"u}hl}}}
\gdef \@abspage@last{27}
